{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how company compares to its peers?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is the detailed income statement breakdow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>world premium penetration in 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the forecasted insurance premium pene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what are the total losses for companies in cou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Phrases\n",
       "0                 how company compares to its peers?\n",
       "1  what is the detailed income statement breakdow...\n",
       "2                  world premium penetration in 2020\n",
       "3  How does the forecasted insurance premium pene...\n",
       "4  what are the total losses for companies in cou..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv_file_path = 'phrases.csv'\n",
    "#df = pd.read_csv(csv_file_path)\n",
    "df = pd.read_csv('C:\\\\Users\\\\Saku\\\\OneDrive\\\\Desktop\\\\text_distance.csv', encoding='latin-1')\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:\\\\Users\\\\Saku\\\\Downloads\\\\GoogleNews-vectors-negative300.bin.gz'\n",
    "\n",
    "wv = KeyedVectors.load_word2vec_format(file_path, binary=True, limit=100000)\n",
    "\n",
    "flat_file_path = 'vectors.txt'\n",
    "wv.save_word2vec_format(flat_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Clean Duplicates, Outliers, and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Saku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Saku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Remove duplicates\n",
    "df_cleaned = df.drop_duplicates(subset=['Phrase'])\n",
    "\n",
    "# Remove outliers (you may customize the criteria)\n",
    "df_cleaned = df_cleaned[df_cleaned['Phrase'].apply(lambda x: len(x.split()) > 1)]\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_cleaned['Cleaned_Phrase'] = df_cleaned['Phrase'].apply(lambda x: ' '.join([word.lower() for word in word_tokenize(x) if word.lower() not in stop_words]))\n",
    "\n",
    "# Display the cleaned dataframe\n",
    "df_cleaned.head()\n",
    "\"\"\"\"\"\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Check if 'Phrase' column exists in the original DataFrame\n",
    "if 'Phrases' not in df.columns:\n",
    "    print(\"Error: 'Phrase' column not found in the DataFrame.\")\n",
    "else:\n",
    "    df_cleaned = df.drop_duplicates(subset=['Phrases'])\n",
    "\n",
    "    df_cleaned = df_cleaned[df_cleaned['Phrases'].apply(lambda x: len(x.split()) > 1)]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    if 'Phrases' in df_cleaned.columns:\n",
    "        df_cleaned['Cleaned_Phrase'] = df_cleaned['Phrases'].apply(lambda x: ' '.join([word.lower() for word in word_tokenize(x) if word.lower() not in stop_words]))\n",
    "        df_cleaned.head()\n",
    "    else:\n",
    "        print(\"Error: 'Phrase' column not found in the cleaned DataFrame.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Assign Word2Vec Embeddings to Cleaned Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m normalized_sum\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate phrase vectors for all cleaned phrases\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m phrase_vectors_cleaned \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([calculate_phrase_vector(phrase) \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaned_Phrase\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "Cell \u001b[1;32mIn[31], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m normalized_sum\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate phrase vectors for all cleaned phrases\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m phrase_vectors_cleaned \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mcalculate_phrase_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaned_Phrase\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "Cell \u001b[1;32mIn[31], line 14\u001b[0m, in \u001b[0;36mcalculate_phrase_vector\u001b[1;34m(phrase)\u001b[0m\n\u001b[0;32m     12\u001b[0m words \u001b[38;5;241m=\u001b[39m phrase\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Get word embeddings, handling missing and similar words\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m word_vectors \u001b[38;5;241m=\u001b[39m [get_word_embedding(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m     15\u001b[0m normalized_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(word_vectors, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(np\u001b[38;5;241m.\u001b[39msum(word_vectors, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m normalized_sum\n",
      "Cell \u001b[1;32mIn[31], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m words \u001b[38;5;241m=\u001b[39m phrase\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Get word embeddings, handling missing and similar words\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m word_vectors \u001b[38;5;241m=\u001b[39m [\u001b[43mget_word_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m     15\u001b[0m normalized_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(word_vectors, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(np\u001b[38;5;241m.\u001b[39msum(word_vectors, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m normalized_sum\n",
      "Cell \u001b[1;32mIn[31], line 7\u001b[0m, in \u001b[0;36mget_word_embedding\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wv[word]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Find the closest similar word using Levenshtein distance\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     closest_match, _ \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mextractOne(word, \u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wv[closest_match]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gensim\\models\\keyedvectors.py:734\u001b[0m, in \u001b[0;36mKeyedVectors.vocab\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvocab\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 734\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe vocab attribute was removed from KeyedVector in Gensim 4.0.0.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    736\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse KeyedVector\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms .key_to_index dict, .index_to_key list, and methods \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    738\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    739\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "def get_word_embedding(word):\n",
    "    # Check for an exact match\n",
    "    if word in wv:\n",
    "        return wv[word]\n",
    "    else:\n",
    "        # Find the closest similar word using Levenshtein distance\n",
    "        closest_match, _ = process.extractOne(word, wv.vocab.keys())\n",
    "        return wv[closest_match]\n",
    "\n",
    "# Function to calculate the approximate phrase vector.\n",
    "def calculate_phrase_vector(phrase):\n",
    "    words = phrase.split()\n",
    "    # Get word embeddings, handling missing and similar words\n",
    "    word_vectors = [get_word_embedding(word) for word in words]\n",
    "    normalized_sum = np.sum(word_vectors, axis=0) / np.linalg.norm(np.sum(word_vectors, axis=0))\n",
    "    return normalized_sum\n",
    "\n",
    "# Calculate phrase vectors for all cleaned phrases\n",
    "phrase_vectors_cleaned = np.array([calculate_phrase_vector(phrase) for phrase in df_cleaned['Cleaned_Phrase']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Batch Execution - Calculate L2 and Cosine Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate L2 distances between all pairs of cleaned phrases\n",
    "l2_distances_cleaned = euclidean_distances(phrase_vectors_cleaned)\n",
    "\n",
    "# Calculate Cosine distances between all pairs of cleaned phrases\n",
    "cosine_distances_cleaned = cosine_distances(phrase_vectors_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: On-the-Fly Execution - Find Closest Match and Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_match(user_input):\n",
    "    cleaned_user_input = ' '.join([word.lower() for word in word_tokenize(user_input) if word.lower() not in stop_words])\n",
    "    user_vector = calculate_phrase_vector(cleaned_user_input)\n",
    "\n",
    "    # Calculate cosine distances between user input and all cleaned phrases\n",
    "    user_distances_cleaned = cosine_distances([user_vector], phrase_vectors_cleaned.flatten().reshape(1, -1))\n",
    "\n",
    "    # Find the index of the closest match\n",
    "    closest_match_index = np.argmin(user_distances_cleaned)\n",
    "\n",
    "    # Extract the closest match and its distance\n",
    "    closest_match_phrase = df_cleaned.loc[closest_match_index, 'Cleaned_Phrase']\n",
    "    closest_match_distance = user_distances_cleaned[0, closest_match_index]\n",
    "\n",
    "    return closest_match_phrase, closest_match_distance\n",
    "\n",
    "# Example usage of on-the-fly execution function\n",
    "user_input_phrase = \"your input phrase here\"\n",
    "closest_match, distance = find_closest_match(user_input_phrase)\n",
    "print(f\"Closest Match: {closest_match}\")\n",
    "print(f\"Distance: {distance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results in a DataFrame\n",
    "results_df_cleaned = pd.DataFrame({\n",
    "    'Phrase1': df_cleaned['Cleaned_Phrase'].repeat(len(df_cleaned)),\n",
    "    'Phrase2': np.tile(df_cleaned['Cleaned_Phrase'], len(df_cleaned)),\n",
    "    'L2_Distance': l2_distances_cleaned.flatten(),\n",
    "    'Cosine_Distance': cosine_distances_cleaned.flatten()\n",
    "})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df_cleaned.to_csv('distances_results_cleaned.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
